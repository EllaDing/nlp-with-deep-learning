\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{float}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}
\definecolor{commentgreen}{rgb}{0.3,0.6,0.75}
\definecolor{keywordgreen}{rgb}{0.15,0.5,0.15}
\definecolor{stringmaroon}{rgb}{0.5,0,0}
\usepackage{amsmath}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{commentgreen},
    keywordstyle=\color{keywordgreen},
    stringstyle=\color{stringmaroon},
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    captionpos=b,                    
    keepspaces=false,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}
\title{Assignment2}
\begin{document}
\maketitle
\section{Question(a)}
We know that:
\begin{equation}
y_w=\left\{\begin{aligned} & 1 &w\text{ shown in context of } o\\
& 0 & w\text{ not shown in context of } o\\
\end{aligned} \right.
\end{equation}
And since y is a one-hot vector, so only for $y_o$, it is 1.
\begin{equation}
\text{cross\_entropy}(y, \hat{y})=\sum_{w \in {V}}y_wlog(\hat{y_w}) = y_olog(\hat{y_o})
\end{equation} 
\section{Question(b)}
$U$ is a matrix where each column is $u_i$ for context word $i$. $c$ is the center word's index, $o$ is the outside word's index.
\begin{equation}
\begin{aligned}
J(o, c, U) &= -log\frac{exp(u_o^Tv_c)}{\sum_{w \in {V}}{exp(u_w^Tv_c)}}\\
&=-u_o^Tv_c + log(\sum_{w \in {V}}{exp(u_w^Tv_c}))
\end{aligned}
\end{equation} 
\begin{equation}
\begin{aligned}
\frac{\partial J(o, c, U)}{\partial v_c} &= -u_o + \frac{\partial log\sum_{w \in V} exp(u_w^Tv_c)}{\partial v_c} \\
&= -u_o + \frac{1}{\sum_{w \in V} exp(u_w^Tv_c)}\frac{\partial{\sum_{w \in V} exp(u_w^Tv_c)}}{\partial v_c}\\ 
&= -u_o + \frac{\sum_{w \in V} u_wexp(u_w^Tv_c)}{\sum_{w \in V} exp(u_w^Tv_c)}\\
&= -Uy + \sum_{w \in V} u_w\hat y\\
&= U(\hat y - y)
\end{aligned}
\end{equation} 
\section{Question(c)}
If $w$ = $o$(this word is the outside word), then:
\begin{equation}
\begin{aligned}
\frac{\partial J(o, c, U)}{\partial u_w} &= -v_c + \frac{\partial log\sum_{w \in V} exp(u_w^Tv_c)}{\partial u_o} \\
&= -v_c + \frac{1}{\sum_{w \in V} exp(u_w^Tv_c)}\frac{\partial{\sum_{w \in V} exp(u_w^Tv_c)}}{\partial u_o}\\ 
&= -v_c + \frac{v_cexp(u_w^Tv_c)}{\sum_{w \in V} exp(u_w^Tv_c)}\\
&= -v_c + v_c\hat y\\
&= v_c(\hat y - y)
\end{aligned}
\end{equation}
Otheriwise $v_c\hat y$
\section{Question(d)}
\begin{equation}
\begin{aligned}
\frac{d\sigma(x)}{d x} &= \frac{d(1+e^{-x})^{-1}}{dx} \\
&= -(1+e^{-x})^{-2} e^{-x} \\
&= -\frac{1}{(1+e^{-x})(1+e^x)} \\
&= \sigma(x)\sigma(-x)
\end{aligned}
\end{equation}
\section{Question(e)}
\begin{equation}
\begin{aligned}
J_{\text{neg-sample}}(v_o, c, U) &= -log(\sigma(u_o^Tv_c)) - \sum_{v_k \in \text{neg}}log(\sigma(-u_k^Tv_c))\\
\frac{\partial J_{\text{neg-sample}}(v_o, c, U)}{\partial v_c} &= -\frac{\partial(log(\sigma(u_o^Tv_c))}{\partial v_c} - \sum_{v_k \in \text{neg}}\frac{\partial log(\sigma(-u_k^Tv_c))}{\partial v_c}\\
&= -u_o\sigma(-u_o^Tv_c) + \sum_{v_k \in \text{neg}}u_k\sigma(u_k^Tv_c)\\
&= u_o[\sigma(u_o^Tv_c)-1] + \sum_{v_k \in \text{neg}}u_k\sigma(u_k^Tv_c)
\end{aligned}
\end{equation} 

\begin{equation}
\begin{aligned}
\frac{\partial J_{\text{neg-sample}}(v_o, c, U)}{\partial u_k} &= -\frac{\partial(log(\sigma(u_o^Tv_c))}{\partial u_k} - \sum_{v_k \in \text{neg}}\frac{\partial log(\sigma(-u_k^Tv_c))}{\partial u_k}\\
&= v_c\sigma(u_k^Tv_c) \quad\text{ if } u_k \ne u_o\\
&= -v_c\sigma(-u_o^Tv_c) \quad\text{ if } u_k = u_o
\end{aligned}
\end{equation} 
This negative-sampling loss function is more efficient to compute than naive softmax loss function because here only one vector $u_k$ or $v_c$ or $u_o$ involved, but in naive softmax, we need the whole matrix $U$ to be involved.
\section{Question(f)}
(i) \begin{equation}
\begin{aligned}
\frac{\partial J_{\text{skip-gram}}(v_o, w_{t-m}, ... , w_{t+m}, U)}{\partial U} &=
\sum_{-m \le j \le m, j \ne 0}\frac{\partial J(v_c, w_{t+j}, U)}{\partial U} \\
&= \sum_{-m \le j \le m, j \ne 0}v_{c}(\hat y_j - y_j) 
\end{aligned}
\end{equation} 
(ii) \begin{equation}
\begin{aligned}
\frac{\partial J_{\text{skip-gram}}(v_o, w_{t-m}, ... , w_{t+m}, U)}{\partial v_c} &= \sum_{-m \le j \le m, j \ne 0} \frac{\partial j(v_c, w_j, U)}{\partial v_c} \\ &= \sum_{-m \le j \le m, j \ne 0}U(\hat y_j - y_j)
\end{aligned}
\end{equation} 
(iii) \begin{equation}
\begin{aligned}
\frac{\partial J_{\text{skip-gram}}(v_o, w_{t-m}, ... , w_{t+m}, U)}{\partial v_w} (\text{ when } w \ne c &)= 0
\end{aligned}
\end{equation} 
\end{document} 

